{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ofzy8xHIXdQF"
   },
   "source": [
    "[Bag of Words Meets Bags of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial/data)\n",
    "======\n",
    "\n",
    "## Data Set\n",
    "\n",
    "The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n",
    "\n",
    "## File descriptions\n",
    "\n",
    "labeledTrainData - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.\n",
    "## Data fields\n",
    "\n",
    "* id - Unique ID of each review\n",
    "* sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "* review - Text of the review\n",
    "\n",
    "## Objective\n",
    "Objective of this dataset is base on **review** we predict **sentiment** (positive or negative) so X is **review** column and y is **sentiment** column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dub6uxUzXdQH"
   },
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "Let's first of all have a look at the data. You can download the file `labeledTrainData.tsv` on the [Kaggle website of the competition](https://www.kaggle.com/c/word2vec-nlp-tutorial/data), or on our [Google Drive](https://drive.google.com/file/d/1a1Lyn7ihikk3klAX26fgO3YsGdWHWoK5/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1WU9XkYXdQI"
   },
   "outputs": [],
   "source": [
    "# Import pandas, numpy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_iH9rAw7XdQL"
   },
   "outputs": [],
   "source": [
    "# Read dataset with extra params sep='\\t', encoding=\"latin-1\"\n",
    "data = pd.read_csv(\"labeledTrainData.tsv\",sep=\"\\t\",encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>3453_3</td>\n",
       "      <td>0</td>\n",
       "      <td>It seems like more consideration has gone into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>5064_1</td>\n",
       "      <td>0</td>\n",
       "      <td>I don't believe they made this film. Completel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>10905_3</td>\n",
       "      <td>0</td>\n",
       "      <td>Guy is a loser. Can't get girls, needs to buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>10194_3</td>\n",
       "      <td>0</td>\n",
       "      <td>This 30 minute documentary BuÃ±uel made in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>8478_8</td>\n",
       "      <td>1</td>\n",
       "      <td>I saw this movie as a child and it broke my he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  sentiment                                             review\n",
       "0       5814_8          1  With all this stuff going down at the moment w...\n",
       "1       2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2       7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3       3630_4          0  It must be assumed that those who praised this...\n",
       "4       9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
       "...        ...        ...                                                ...\n",
       "24995   3453_3          0  It seems like more consideration has gone into...\n",
       "24996   5064_1          0  I don't believe they made this film. Completel...\n",
       "24997  10905_3          0  Guy is a loser. Can't get girls, needs to buil...\n",
       "24998  10194_3          0  This 30 minute documentary BuÃ±uel made in the...\n",
       "24999   8478_8          1  I saw this movie as a child and it broke my he...\n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbuaIdhWXdQO"
   },
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJr6juwHXdQP",
    "outputId": "f2cd5751-6c3c-4259-8f20-d3caf34679ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop words\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 65971),\n",
       " ('/><br', 50935),\n",
       " ('The', 33760),\n",
       " ('movie', 30496),\n",
       " ('film', 27394),\n",
       " ('one', 20685),\n",
       " ('like', 18133),\n",
       " ('This', 12279),\n",
       " ('would', 11922),\n",
       " ('good', 11435),\n",
       " ('It', 10950),\n",
       " ('really', 10814),\n",
       " ('even', 10605),\n",
       " ('see', 10154),\n",
       " ('-', 9355),\n",
       " ('get', 8776),\n",
       " ('story', 8523),\n",
       " ('much', 8506),\n",
       " ('time', 7762),\n",
       " ('make', 7485)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "vocab = Counter()\n",
    "for twit in data.review:\n",
    "    for word in twit.split(' '):\n",
    "        vocab[word] += 1\n",
    "\n",
    "vocab.most_common(20)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "vocab_reduced = Counter()\n",
    "\n",
    "for w, c in vocab.items():\n",
    "    if not w in stop_words:\n",
    "        vocab_reduced[w]=c\n",
    "\n",
    "vocab_reduced.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3v220Tp_XdQS"
   },
   "outputs": [],
   "source": [
    "# Removing special characters and \"trash\"\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def preprocessor(text):\n",
    "    # Remove HTML markup\n",
    "    text = strip_html(text)\n",
    "    \n",
    "    # Save emoticons for later appending\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    \n",
    "    # Remove any non-word character and append the emoticons,\n",
    "    # removing the nose character for standarization. Convert to lower case\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-o5ED67LXdQV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Your', 'doing', 'makes', 'things', 'very', 'interested']\n",
      "['your', 'do', 'make', 'thing', 'veri', 'interest']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer and stemming\n",
    "# tokenizer: to break down our twits in individual words\n",
    "# stemming: reducing a word to its root\n",
    "from nltk.stem import PorterStemmer\n",
    "# Your code here\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "print(tokenizer('Your doing makes things very interested'))\n",
    "print(tokenizer_porter('Your doing makes things very interested'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zjzOxu4MXdQZ"
   },
   "outputs": [],
   "source": [
    "# split the dataset in train and test\n",
    "# Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['review']\n",
    "y = data['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22721    Have just finished watching this film, which u...\n",
       "4463     A blockbuster at the time of it's original rel...\n",
       "9686     I love bad movies. Not only, because they ofte...\n",
       "12558    The brilliance of this movie is that even a co...\n",
       "6448     I just have to say, this is one of my favorite...\n",
       "1998     This was, so far, the worst movie I have seen ...\n",
       "6017     This is one of the better classic Edgar Wallac...\n",
       "8713     Moon Child was one of the more symbolic movies...\n",
       "3509     Two years later... Bill (Alex Winter) and Ted ...\n",
       "23183    Quite one of the worst films I have ever seen....\n",
       "21621    Apparently most viewer knows nothing about the...\n",
       "1617     I'd never seen an independent movie and I was ...\n",
       "5053     This was fun to watch, spookily atmospheric an...\n",
       "14121    What a waste of time and money! My hubby and I...\n",
       "19256    I did enjoy this film, I thought it ended up b...\n",
       "24603    \\A Guy Thing\\\" tries to capture the feeling of...\n",
       "15110    I feel like I'm the only kid in town who was a...\n",
       "3230     I saw this movie when I was little - It was ca...\n",
       "13616    \\Shall We Dance?\\\", a light-hearted flick from...\n",
       "496      In what could have been seen as a coup towards...\n",
       "10619    This is a film of immense appeal to a relative...\n",
       "13941    I picked up this movie with the intention of g...\n",
       "13990    All right I recently got a chance to rent this...\n",
       "3447     I've been waiting for a superhero movie like t...\n",
       "7205     Kathryn Bigelow and Mark Boal are already prep...\n",
       "3583     Without question, this film has to be one of t...\n",
       "8425     Story starts slow and nothing funny happens fo...\n",
       "5438     The Legend of Zu, as I saw it, was a very inte...\n",
       "2347     This movie is the biggest waste of nine dollar...\n",
       "17933    This movie features Charlie Spradling dancing ...\n",
       "1202     Kept my attention from start to finish. Great ...\n",
       "10488    This truly funny movie has a zany cast of char...\n",
       "4508     Basically the exact same movie as \\House of Wa...\n",
       "5844     Loved this show...smart acting, smart dialog, ...\n",
       "16434    Stardust Another Guarded Review (originally wr...\n",
       "23534    THE FOX AND THE CHILD is the latest film from ...\n",
       "6542     I can't understand all the hype about this mov...\n",
       "6458     This movie is a great. The plot is very true t...\n",
       "7949     No-nonsense Inspector Hollaway (a solid turn b...\n",
       "14915    When I first read Hamlet, I couldn't help but ...\n",
       "18326    My wife and I found this film to be highly uns...\n",
       "17452    I finally purchased and added to my collection...\n",
       "8221     Realistic movie,sure,except for the fact that ...\n",
       "9374     An absolutely atrocious adaptation of the wond...\n",
       "21494    This film and it's sequel Barry Mckenzie holds...\n",
       "9260     If this movie had not been labeled a Disney pi...\n",
       "556      I need to be honest. I watched and enjoy this ...\n",
       "7434     This is a very funny Ealing comedy about a com...\n",
       "21657    Hilarious hardly begins to describe this one o...\n",
       "11495    Long on action and stunt work, but so short on...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COr1xR7PXdQc"
   },
   "source": [
    "## 3. Create Model and Train \n",
    "\n",
    "Using **Pipeline** to concat **tfidf** step and **LogisticRegression** step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOpwINJmXdQd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 TfidfVectorizer(preprocessor=<function preprocessor at 0x000002905AEB5438>,\n",
       "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
       "                                             'our', 'ours', 'ourselves', 'you',\n",
       "                                             \"you're\", \"you've\", \"you'll\",\n",
       "                                             \"you'd\", 'your', 'yours',\n",
       "                                             'yourself', 'yourselves', 'he',\n",
       "                                             'him', 'his', 'himself', 'she',\n",
       "                                             \"she's\", 'her', 'hers', 'herself',\n",
       "                                             'it', \"it's\", 'its', 'itself', ...],\n",
       "                                 tokenizer=<function tokenizer_porter at 0x00000290564D1318>)),\n",
       "                ('clf', LogisticRegression(random_state=0))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Pipeline, LogisticRegression, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words,\n",
    "                        tokenizer=tokenizer_porter,\n",
    "                        preprocessor=preprocessor)\n",
    "\n",
    "clf = Pipeline([('vect', tfidf),\n",
    "                ('clf', LogisticRegression(random_state=0))])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYdoVMx_XdQf"
   },
   "source": [
    "## 4. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2ZOzHoaXdQg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8854\n",
      "confusion matrix:\n",
      " [[2164  339]\n",
      " [ 234 2263]]\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88      2503\n",
      "           1       0.87      0.91      0.89      2497\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Test dataset to evaluate model\n",
    "# classification_report\n",
    "# confusion matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Now apply those above metrics to evaluate your model\n",
    "# Your code here\n",
    "predictions = clf.predict(X_test)\n",
    "print('accuracy:',accuracy_score(y_test,predictions))\n",
    "print('confusion matrix:\\n',confusion_matrix(y_test,predictions))\n",
    "print('classification report:\\n',classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the worst movie i have ever seen --> Negative, Positive = [0.98786448 0.01213552]\n",
      "I watched this film so many time --> Negative, Positive = [0.21778856 0.78221144]\n",
      "Masterpiece. :( --> Negative, Positive = [0.26044005 0.73955995]\n",
      "What a waste of time watching something like this. --> Negative, Positive = [0.98594185 0.01405815]\n",
      "This is so bad. Now i have to watch it again --> Negative, Positive = [0.99598855 0.00401145]\n",
      "This is a movie. --> Negative, Positive = [0.68809801 0.31190199]\n"
     ]
    }
   ],
   "source": [
    "# twits = [\n",
    "#     \"This is the worst movie i have ever seen\",\n",
    "#     'I watched this film so many time',\n",
    "#     'Masterpiece. :(',\n",
    "#     'What a waste of time watching something like this.',\n",
    "#     'This is so bad. Now i have to watch it again',\n",
    "#     \"This is a movie.\"\n",
    "# ]\n",
    "\n",
    "# preds = clf.predict_proba(twits)\n",
    "\n",
    "# for i in range(len(twits)):\n",
    "#     print(f'{twits[i]} --> Negative, Positive = {preds[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NCAuLC0aXdQi"
   },
   "source": [
    "## 5. Export Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYHo_x67XdQj"
   },
   "outputs": [],
   "source": [
    "# Using pickle to export our trained model\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "pickle.dump(clf, open('logisticRegression.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week5_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
